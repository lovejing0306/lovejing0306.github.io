---
layout: post
title: LeNet
categories: [Classification]
description: LeNet
keywords: Classification
---


分类模型 LeNet
---



## 背景
&emsp;&emsp;$LeNet-5$ 出自论文$Gradient-Based \ Learning \ Applied\ to \ Document \ Recognition$，是一种用于手写体字符识别的非常高效的卷积神经网络。虽然 $LeNet-5$ 是个小网络，但是它包含了深度学习的基本模块：卷积层、池化层、全链接层，是其他深度学习模型的基础。

&emsp;&emsp; $LeNet-5$ 共有 $7$ 层，不包含输入，每层都包含可训练参数；每个层有多个 $Feature Map$，每个 $FeatureMap$ 通过一种卷积滤波器提取输入的一种特征，每个 $FeatureMap$ 有多个神经元。

<center>
    <img 
    src="https://github.com/lovejing0306/Images/blob/master/DeepLearning/Model/LeNet/lenet1.jpg?raw=true"
    width="640" height="" />
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">letnet</div>
</center>

## 参数详解
> 输入层-input层

```
输入层将图像的尺寸统一归一化为32*32。
注意：本层不算LeNet-5的网络结构，传统上，不将输入层视为网络层次结构之一。
```
> 卷积层-C1层

```
输入：32*32
卷积核大小：5*5
卷积核种类：6
输出featuremap大小：28*28 (32-5+1)=28
神经元数量：28*28*6
训练参数：(5*5+1)*1*6（每个滤波器5*5=25个unit参数和一个bias参数，一共6个滤波器）
连接数：(5*5+1)*6*28*28=122304

详细说明：
对输入图像进行第一次卷积运算（使用 6 个大小为 5*5 的卷积核），得到6个C1特征图（6个大小为28*28的 feature maps,32-5+1=28）。
再来看看需要多少个参数，卷积核的大小为5*5，总共就有6*（5*5+1）=156个参数，其中+1是表示一个核有一个bias。
对于卷积层C1，C1内的每个像素都与输入图像中的5*5个像素和1个bias有连接，所以总共有156*28*28=122304个连接（connection）。有122304个连接，但是我们只需要学习156个参数，主要是通过权值共享实现的。
```
> 池化层-S2层

```
输入：28*28
采样区域：2*2
采样方式：4个输入相加，乘以一个可训练参数，再加上一个可训练偏置，结果通过sigmoid
采样种类：6
输出featureMap大小：14*14（28/2）
神经元数量：14*14*6
连接数：（2*2+1）*6*14*14
S2中每个特征图的大小是C1中特征图大小的1/4。

详细说明：
第一次卷积之后紧接着就是池化运算，使用 2*2核 进行池化，于是得到了S2，6个14*14的 特征图（28/2=14）。
S2这个pooling层是对C1中的2*2区域内的像素求和乘以一个权值系数再加上一个偏置，然后将这个结果再做一次映射。
同时有5x14x14x6=5880个连接。
```
> **卷积层-C3层**

```
输入：S2中所有6个或者几个特征map组合
卷积核大小：5*5
卷积核种类：16
输出featureMap大小：10*10  (14-5+1)=10
可训练参数：6*(3*5*5+1)+6*(4*5*5+1)+3*(4*5*5+1)+1*(6*5*5+1)=1516
    C3中的每个特征map是连接到S2中的所有6个或者几个特征map的，表示本层的特征map是上一层提取到的特征map的不同组合。
    存在的一个方式是：C3的前6个特征图以S2中3个相邻的特征图子集为输入。接下来6个特征图以S2中4个相邻特征图子集为输入。然后的3个以不相邻的4个特征图子集为输入。最后一个将S2中所有特征图为输入。
连接数：10*10*1516=151600

详细说明：
采用这种卷积方式的原因，论文中提到：
1、减少参数
2、这种不对称的组合连接的方式有利于提取多种组合特征
```
> 池化层-S4层

```
输入：10*10
采样区域：2*2
采样方式：4个输入相加，乘以一个可训练参数，再加上一个可训练偏置。结果通过sigmoid
采样种类：16
输出featureMap大小：5*5  (10/2)
神经元数量：5*5*16=400
连接数：16*(2*2+1)*5*5=2000
S4中每个特征图的大小是C3中特征图大小的1/4

详细说明：
S4是pooling层，窗口大小仍然是2*2，共计16个feature map，C3层的16个10x10的图分别进行以2x2为单位的池化得到16个5x5的特征图。
有5x5x5x16=2000个连接。连接的方式与S2层类似。
```
> 卷积层-C5层

```
输入：S4层的全部16个单元特征map（与s4全相连）
卷积核大小：5*5
卷积核种类：120
输出featureMap大小：1*1（5-5+1）
可训练参数/连接：120*（16*5*5+1）=48120
```

> 全连接层-F6层

```
输入：c5 120维向量
计算方式：计算输入向量和权重向量之间的点积，再加上一个偏置，结果通过sigmoid函数输出。
输出向量大小：84
可训练参数:84*(120+1)=10164
```
> 全连接层-output层

```
    Output层也是全连接层，共有10个节点，分别代表数字0到9，且如果节点i的值为0，则网络识别的结果是数字i。
采用的是径向基函数（RBF）的网络连接方式。假设x是上一层的输入，y是RBF的输出，则RBF输出的计算方式是：
```
$$
{ y }_ { i }=\sum _ { j=1 }^{ n }{ { \left( { x }_{ j }-{ w }_{ ji } \right)  }^{ 2 } } \quad i\in \left\{ 0,1...,9 \right\} \quad j\in \left\{ 0,1,...,83 \right\} 
$$

```
    输出层是由欧式径向基单元（Euclidean Radial Badi）组成，每类一个单元，每个单元有84个输入。
每个输出ERBF单元计算输入向量和参数向量之间的欧式距离。输入离参数向量越远，ERBF输出越大。
因此，一个ERBF输出可以被理解为衡量输入模式与ERBF相关联类的一个模型的匹配程度的惩罚项。
同时ERBF参数向量起着F6层目标向量的角色。这些向量的成分是+1或者-1，也可以防止F6层的Sigmoid函数饱和。
```

## 总结
* 输入图像大小为 $32 \times 32$
* 卷积核大小为 $5 \times 5$
* 池化核大小为 $2 \times 2$
* 使用 $sigmoid$ 激活函数
* 输出层为欧式径向基单元
* 网络总共有 $7$ 层
* 每个卷积层后接一个池化层