---
layout: post
title: 泛化能力
categories: [MachineLearning]
description: 泛化能力
keywords: MachineLearning
---


泛化能力
---


&emsp;&emsp;通常利用最小化训练误差来训练模型，但是真正关心的是测试误差。因此通过测试误差来评估模型的泛化能力。
1. 训练误差是模型在训练集上的平均损失，其大小虽然有意义，但是本质上不重要。
2. 测试误差是模型在测试集上的平均损失，反应了模型对未知数据集的预测能力。

&emsp;&emsp;模型对未知数据的预测能力称作模型的泛化能力，它是模型最重要的性质。泛化误差可以反映模型的泛化能力：泛化误差越小，该模型越有效。假设训练集和测试集共同的、潜在的样本分布称作数据生成分布，记作 $p  _  {data}(x,y)$  。则泛化误差定义为模型的期望风险，即：

$$
R_{exp}(f)=\mathbb E[L(y, f(x))]=\int_{\mathcal{X \times Y}}L(y, f(x))p_{data}(x,y)dxdy
$$

通常泛化误差是不可知的，因为无法获取联合概率分布 $p  _  {data}(x,y)$  以及无限的采样点。现实中通常利用测试误差评估模型的泛化能力。由于测试数据集是有限的，因此这种评估结果不完全准确。

&emsp;&emsp;统计理论表明：如果训练集和测试集中的样本都是独立同分布产生的，则有模型的训练误差的期望等于模型的测试误差的期望。

&emsp;&emsp;机器学习的“没有免费的午餐定理”表明：在所有可能的数据生成分布上，没有一个机器学习算法总是比其他的要好。该结论仅在考虑所有可能的数据分布时才成立。现实中特定任务的数据分布往往满足某类假设，从而可以设计在这类分布上效果更好的学习算法。这意味着机器学习并不需要寻找一个通用的学习算法，而是寻找一个在关心的数据分布上效果最好的算法。

&emsp;&emsp;正则化是对学习算法做的一个修改，这种修改趋向于降低泛化误差（而不是降低训练误差）。正则化是机器学习领域的中心问题之一。没有免费的午餐定理说明了没有最优的学习算法，因此也没有最优的正则化形式。
