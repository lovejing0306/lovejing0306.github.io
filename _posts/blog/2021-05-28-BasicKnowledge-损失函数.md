---
layout: post
title: 损失函数
categories: [BasicKnowledge]
description: 损失函数
keywords: BasicKnowledge
---


深度学习基础知识点损失函数
---


## 回归损失

### 均方误差（MSE）
均方误差又称平方损失或$L2$损失，其度量的是预测值和实际观测值间差的平方的均值。它只考虑误差的平均大小，不考虑其方向。但由于经过平方，与真实值偏离较多的预测值会比偏离较少的预测值受到更为严重的惩罚。再加上 $MSE$ 的数学特性很好，这使得计算梯度变得更容易。

$$
MSE=\frac{1}{n}\sum_{i=1}^n{\left( y_i-\widehat{y}_ i \right)}^2
$$

### 平均绝对误差（MAE）
平均绝对误差又称 $L1$，其损失度量的是预测值和实际观测值之间绝对差之和的平均值。和$MSE$ 一样，这种度量方法也是在不考虑方向的情况下衡量误差大小。但和$MSE$ 的不同之处在于，$MAE$ 需要像线性规划这样更复杂的工具来计算梯度。此外，$MAE$ 对异常值更加稳健，因为它不使用平方。

$$
MAE=\frac{1}{n}\sum_{i=1}^n{\left| y_i-\widehat{y}_ i \right|}
$$

### 平均偏差误差（MBE）
平均偏差误差在机器学习领域没有那么常见，它与$MAE$ 相似，唯一的区别是这个函数没有用绝对值。需要注意的是该函数会使正负误差互相抵消。尽管在实际应用中没那么准确，但它可以确定模型存在正偏差还是负偏差。

$$
MBE=\frac{1}{n}\sum_{i=1}^n{\left( y_i-\widehat{y}_ i \right)}
$$

## 分类损失
### Hinge Loss
$Hinge \ Loss$ 常用于最大间隔分类，用在支持向量机中的公式表示如下：

$$
Loss=\sum_{j\ne y_j}{\max \left( 0,s_j-s_{y_i}+1 \right)}
$$

在一定的安全间隔内（通常是 $1$），正确类别的分数应高于所有错误类别的分数之和。尽管不可微，但它是一个凸函数。

### 交叉熵损失（Cross Entropy）
交叉熵损失又称负对数似然损失，随着预测概率偏离实际标签，交叉熵损失会逐渐增加。

$$
CE=-\left( y_i\log \widehat{y}_ i+\left( 1-y_ i \right) \log \left( 1-\widehat{y}_ i \right) \right) 
$$

注意，当实际标签为 $1(y(i)=1)$ 时，函数的后半部分消失；而当实际标签是为 $0(y(i=0))$ 时，函数的前半部分消失。简言之，我们只是把对真实值类别的实际预测概率的对数相乘。此外，交叉熵损失会重重惩罚那些置信度高但是错误的预测值。